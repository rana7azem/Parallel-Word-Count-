# ğŸ§® Parallel Word Count Project (Phase 1)

**Author:** Rana  
**Course:** Parallel & Distributed Computing  
**Phase:** 1 â€” Parallelization and Performance Analysis  

---

## ğŸ§  Project Overview

The goal of this project is to implement a **Word Count** application using:
- A **sequential (serial)** version.
- A **parallel** version based on **OpenMP**.

The program reads a large text file, tokenizes all words (case-insensitive), counts their occurrences, and prints the top frequent words.

---

## ğŸ§© Implementation Details

### ğŸ§± Sequential Version (`src/wc_serial.cpp`)
- Reads the file line by line.
- Splits words on non-alphanumeric characters.
- Converts all characters to lowercase.
- Uses `unordered_map<string, long long>` to store word frequencies.
- Prints results alphabetically.

### âš™ï¸ Parallel Version (`src/wc_parallel.cpp`)
- Uses **OpenMP** for multithreading.
- Each thread builds a **local map** to avoid race conditions.
- After processing, all local maps are **merged serially** into a global map.
- Parallel section:
  ```cpp
  #pragma omp parallel
  {
      int tid = omp_get_thread_num();
      auto &mp = local_maps[tid];
      #pragma omp for schedule(static)
      for (...) { /* tokenize & count */ }
  }


  ğŸ§® Correctness Check

To verify correctness between serial and parallel versions:

./wc_serial.exe data/big.txt | sort > out_serial.txt
OMP_NUM_THREADS=4 ./wc_parallel.exe data/big.txt 100000 | sort > out_parallel.txt
diff out_serial.txt out_parallel.txt


âœ… If no output, both versions match perfectly.

ğŸ§  Avoiding Race Conditions

Each thread maintains its own unordered_map â†’ No shared writes.

Final merging happens after all threads finish.

Safer and faster than using #pragma omp critical on shared map.

âš¡ Performance Results
ğŸ§¾ Timing Data
Threads (p)	T<sub>P</sub> (ms)	T<sub>P</sub> (s)	Speedup (T<sub>S</sub>/T<sub>P</sub>)	Efficiency (Speedup/p)
1	225.595	0.2256	1.108	1.108
2	135.667	0.1357	1.842	0.921
4	81.594	0.0816	3.064	0.766
8	61.976	0.0620	4.032	0.504

Serial baseline T<sub>S</sub> â‰ˆ 0.250 s

ğŸ“ˆ Speedup & Efficiency Graphs
Metric	Plot
Speedup vs Threads	


Efficiency vs Threads	
ğŸ“Š Analysis Summary
ğŸ§© Observations

Speedup improves as threads increase, but efficiency decreases gradually.

At 8 threads â†’ ~4Ã— speedup (â‰ˆ 50% efficiency).

Caused by:

File I/O being serial.

Merge phase overhead.

Thread synchronization and cache effects.

âš–ï¸ Amdahlâ€™s Law

Even if 90% of a program is parallelized, the remaining serial part limits total speedup.

This matches the observed results â€” scaling is sub-linear due to the serial fraction and OpenMP overhead.

ğŸ§° Build & Run (Windows/MSYS2 UCRT64)
ğŸ›  Build
g++ -O3 -std=c++17 src/wc_serial.cpp -o wc_serial.exe -Wl,-subsystem,console
g++ -O3 -std=c++17 -fopenmp src/wc_parallel.cpp -o wc_parallel.exe -Wl,-subsystem,console

â–¶ï¸ Run
./wc_serial.exe data/big.txt
OMP_NUM_THREADS=4 ./wc_parallel.exe data/big.txt 10

ğŸ§¾ References

OpenMP API Specification v5.2

Amdahl, G. M. (1967). Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities
